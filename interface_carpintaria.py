import streamlit as st
import os
from smolagents import CodeAgent, LiteLLMModel, tool

# --- 1. CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(page_title="Carpintaria Digital Pro", page_icon="ü™ö", layout="wide")
st.title("ü™ö Carpintaria Digital Pro (Vers√£o H√≠brida 2.5)")

# --- IMPORTA√á√ÉO SEGURA DO OLLAMA (OFFLINE) ---
try:
    import ollama
    # Testa conex√£o r√°pida
    ollama.list()
    OLLAMA_AVAILABLE = True
except Exception:
    OLLAMA_AVAILABLE = False

# --- IMPORTA√á√ÉO SEGURA DA BUSCA (ONLINE) ---
try:
    from duckduckgo_search import DDGS
    BUSCA_DISPONIVEL = True
except ImportError:
    BUSCA_DISPONIVEL = False

from ferramentas_avancadas import consultar_documentos, salvar_arquivo, ler_arquivo

# --- FERRAMENTA DE BUSCA CORRIGIDA (Com documenta√ß√£o estrita) ---
@tool
def buscar_na_web(termo: str) -> str:
    """
    Pesquisa na internet (DuckDuckGo) para encontrar informa√ß√µes em tempo real.
    Use esta ferramenta quando precisar de pre√ßos atuais, cota√ß√£o do d√≥lar ou not√≠cias.

    Args:
        termo: O texto da pesquisa ou pergunta a ser buscada no DuckDuckGo.
    """
    if not BUSCA_DISPONIVEL:
        return "Erro: Biblioteca de busca n√£o instalada no sistema."
    
    try:
        # Tenta conectar. Se estiver sem net, vai cair no except
        results = DDGS().text(termo, max_results=3)
        if not results:
            return "Nenhum resultado encontrado."
        
        resposta = f"Resultados para '{termo}':\n"
        for i, r in enumerate(results):
            resposta += f"{i+1}. {r['title']}: {r['body']} (Link: {r['href']})\n"
        return resposta
    except Exception as e:
        # Retorna erro amig√°vel em vez de quebrar o app
        return f"‚ö†Ô∏è Falha na busca (Poss√≠vel falta de internet): {str(e)}"

# --- 2. BARRA LATERAL (MENU) ---
with st.sidebar:
    st.header("üß† C√©rebro da IA")
    
    opcoes_modelos = {}

    # --- GOOGLE GEMINI (Nomes Novos) ---
    st.caption("‚òÅÔ∏è Google (Requer Internet)")
    opcoes_modelos["Google: Gemini 2.5 Flash (Novo!)"] = ("gemini/gemini-2.5-flash", "GEMINI_API_KEY")
    opcoes_modelos["Google: Gemini 2.5 Pro (Potente)"] = ("gemini/gemini-2.5-pro", "GEMINI_API_KEY")
    opcoes_modelos["Google: Gemini 2.0 Flash Lite"] = ("gemini/gemini-2.0-flash-lite", "GEMINI_API_KEY")

    # --- GROQ ---
    st.caption("‚òÅÔ∏è Groq (Gr√°tis)")
    opcoes_modelos["Groq: Llama 3.3 (Vers√°til)"] = ("groq/llama-3.3-70b-versatile", "GROQ_API_KEY")

    # --- OPENROUTER ---
    st.caption("‚òÅÔ∏è OpenRouter")
    opcoes_modelos["OpenRouter: DeepSeek R1 (Free)"] = ("openrouter/deepseek/deepseek-r1:free", "OPENROUTER_API_KEY")
    opcoes_modelos["OpenRouter: Mistral 7B (Free)"] = ("openrouter/mistralai/mistral-7b-instruct:free", "OPENROUTER_API_KEY")

    st.divider()

    # --- LOCAL (OLLAMA) ---
    if OLLAMA_AVAILABLE:
        st.success("üü¢ Modo Local (Offline) Ativo")
        # Adiciona modelos locais no topo da lista
        opcoes_locais = {
            "üè† Local: Qwen 2.5 Coder": ("ollama/qwen2.5-coder:3b", None),
            "üè† Local: Llama 3.2": ("ollama/llama3.2:latest", None),
        }
        # Junta os dicion√°rios (Locais primeiro)
        opcoes_modelos = {**opcoes_locais, **opcoes_modelos}
    else:
        st.error("üî¥ Modo Local Indispon√≠vel (Rode 'ollama serve')")

    # Sele√ß√£o
    nome_escolhido = st.selectbox("Escolha o C√©rebro:", list(opcoes_modelos.keys()))
    model_id, api_env_var = opcoes_modelos[nome_escolhido]

    modo_agente = st.toggle("üïµÔ∏è Agente (Docs + Web)", value=True)

    if st.button("üóëÔ∏è Limpar Conversa"):
        st.session_state["messages"] = []
        st.rerun()

# --- 3. L√ìGICA DO CHAT ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Pergunte algo..."):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        status = st.status(f"‚öôÔ∏è Processando com {nome_escolhido}...", expanded=True)

        try:
            # 1. Configura Chave API (S√≥ se n√£o for Local)
            api_key = None
            if api_env_var: 
                api_key = os.environ.get(api_env_var)
                if not api_key:
                    status.update(label="‚ùå Sem Chave", state="error")
                    st.error(f"Falta a chave {api_env_var}!")
                    st.stop()
            
            # 2. Configura URL Local (S√≥ se for Ollama)
            base_url = "http://localhost:11434" if "ollama" in model_id else None
            
            modelo_agente = LiteLLMModel(
                model_id=model_id,
                api_key=api_key, 
                api_base=base_url,
                max_tokens=4000
            )

            if modo_agente:
                minhas_ferramentas = [consultar_documentos, salvar_arquivo, ler_arquivo]
                
                # S√≥ usa a busca se a biblioteca existir
                if BUSCA_DISPONIVEL:
                    minhas_ferramentas.append(buscar_na_web)

                agent = CodeAgent(
                    tools=minhas_ferramentas, 
                    model=modelo_agente, 
                    add_base_tools=True,
                    additional_authorized_imports=['datetime', 'numpy', 'pandas', 'os', 'json', 'duckduckgo_search']
                )
                
                # INSTRU√á√ïES BLINDADAS
                aviso_offline = ""
                if "Local" in nome_escolhido:
                    aviso_offline = "VOC√ä EST√Å EM MODO LOCAL. Se a ferramenta 'buscar_na_web' falhar, ignore e responda com seu conhecimento interno."

                prompt_sistema = f"""
                SOLICITA√á√ÉO: {prompt}
                DIRETRIZES:
                1. Priorize 'consultar_documentos' para perguntas da empresa.
                2. Use 'buscar_na_web' para dados externos.
                {aviso_offline}
                3. Responda sempre em Portugu√™s.
                """
                
                resposta_final = agent.run(prompt_sistema)
            else:
                agent = CodeAgent(tools=[], model=modelo_agente, add_base_tools=False)
                resposta_final = agent.run(prompt)

            status.update(label="‚úÖ Pronto!", state="complete", expanded=False)
            message_placeholder.markdown(resposta_final)
            st.session_state["messages"].append({"role": "assistant", "content": resposta_final})

        except Exception as e:
            status.update(label="‚ùå Erro", state="error")
            st.error(f"Erro t√©cnico: {str(e)}")
