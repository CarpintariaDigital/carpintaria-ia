import streamlit as st
import os
from smolagents import CodeAgent, LiteLLMModel, DuckDuckGoSearchTool

# --- BLOCO DE IMPORTA√á√ÉO SEGURA (Ollama) ---
try:
    import ollama
    # Tenta listar modelos s√≥ para ver se o servidor responde
    ollama.list()
    OLLAMA_AVAILABLE = True
except Exception:
    OLLAMA_AVAILABLE = False

# --- IMPORTS DOS NOSSOS M√ìDULOS ---
from ferramentas_avancadas import consultar_documentos, salvar_arquivo, ler_arquivo

# Inicializa ferramenta de busca na internet
ferramenta_busca = DuckDuckGoSearchTool()

# --- 1. CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(page_title="Carpintaria Digital Pro", page_icon="ü™ö", layout="wide")
st.title("ü™ö Carpintaria Digital Pro")

# --- 2. PAINEL DE CONTROLE (SIDEBAR) ---
with st.sidebar:
    st.header("üß† C√©rebro da IA")
    
    # --- MENU DE ESCOLHA DE MODELOS ---
    # Estrutura: "Nome no Menu": ("provedor/modelo", "nome_da_variavel_api")
    
    opcoes_modelos = {
        # 1. GROQ (Melhor op√ß√£o Gratuita e R√°pida)
        "üöÄ Groq: Llama 3.3 (Recomendado)": ("groq/llama-3.3-70b-versatile", "GROQ_API_KEY"),
        
        # 2. GOOGLE (Corre√ß√£o do nome do modelo)
        "‚òÅÔ∏è Google: Gemini 1.5 Flash": ("gemini/gemini-1.5-flash", "GEMINI_API_KEY"),
        
        # 3. OPENROUTER (Apenas modelos explicitamente 'free')
        "üÜì OpenRouter: Google Gemini 2.0 (Free)": ("openrouter/google/gemini-2.0-flash-exp:free", "OPENROUTER_API_KEY"),
        "üÜì OpenRouter: Llama 3 8B (Free)": ("openrouter/meta-llama/llama-3-8b-instruct:free", "OPENROUTER_API_KEY"),
    }

    # Se o Ollama estiver rodando (Local), adiciona as op√ß√µes locais
    if OLLAMA_AVAILABLE:
        opcoes_modelos["üè† Local: Qwen 2.5 Coder"] = ("ollama/qwen2.5-coder:3b", None)
        opcoes_modelos["üè† Local: Llama 3.2"] = ("ollama/llama3.2:latest", None)
        st.success("üü¢ Modo Local Ativo")
    
    # O Menu Dropdown
    nome_escolhido = st.selectbox("Escolha o Modelo:", list(opcoes_modelos.keys()))
    
    # Pega as configura√ß√µes baseadas na escolha
    model_id, api_env_var = opcoes_modelos[nome_escolhido]

    st.divider()
    
    modo_agente = st.toggle("üïµÔ∏è Ativar Agente (Busca + Docs)", value=True)
    st.caption("Ferramentas: Internet, PDFs, Arquivos")

    if st.button("üóëÔ∏è Limpar Mem√≥ria"):
        st.session_state["messages"] = []
        st.rerun()

# --- 3. HIST√ìRICO ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 4. L√ìGICA PRINCIPAL ---
if prompt := st.chat_input("Pergunte sobre madeira, pre√ßos ou documentos..."):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        status = st.status(f"‚öôÔ∏è Conectando ao {nome_escolhido}...", expanded=True)

        try:
            # --- PREPARA√á√ÉO DAS CHAVES API ---
            api_key = None
            if api_env_var: # Se for modelo de nuvem
                api_key = os.environ.get(api_env_var)
                if not api_key:
                    status.update(label="‚ùå Erro de Chave", state="error")
                    st.error(f"Falta a chave {api_env_var} nos Secrets do Streamlit!")
                    st.stop()
            
            # --- CONFIGURA√á√ÉO DO MODELO ---
            # Define URL base se for Ollama
            base_url = "http://localhost:11434" if "ollama" in model_id else None
            
            modelo_agente = LiteLLMModel(
                model_id=model_id,
                api_key=api_key, 
                api_base=base_url,
                max_tokens=4000,
                temperature=0.2
            )

            # --- MODO AGENTE OU CHAT ---
            if modo_agente:
                # Agente com ferramentas (Internet + Docs)
                agent = CodeAgent(
                    tools=[consultar_documentos, salvar_arquivo, ler_arquivo, ferramenta_busca], 
                    model=modelo_agente, 
                    add_base_tools=True,
                    additional_authorized_imports=['datetime', 'numpy', 'pandas', 'os', 'json']
                )
                
                prompt_sistema = f"""
                SOLICITA√á√ÉO: {prompt}
                DIRETRIZES:
                1. Use 'duckduckgo_search' para coisas atuais (pre√ßos, c√¢mbio, not√≠cias).
                2. Use 'consultar_documentos' para dados internos da empresa.
                3. Responda sempre em Portugu√™s.
                """
                resposta_final = agent.run(prompt_sistema)
            else:
                # Agente simples (Conversa r√°pida)
                agent = CodeAgent(tools=[], model=modelo_agente, add_base_tools=False)
                resposta_final = agent.run(prompt)

            status.update(label="‚úÖ Conclu√≠do!", state="complete", expanded=False)
            message_placeholder.markdown(resposta_final)
            st.session_state["messages"].append({"role": "assistant", "content": resposta_final})

        except Exception as e:
            status.update(label="‚ùå Erro", state="error")
            erro_msg = str(e)
            
            # Tratamento de erros amig√°vel
            if "404" in erro_msg and "gemini" in erro_msg.lower():
                st.error("Erro do Google: Modelo n√£o encontrado. Tente selecionar o Groq.")
            elif "402" in erro_msg or "credits" in erro_msg.lower():
                st.error("Erro do OpenRouter: Conta sem cr√©ditos. Use o Groq ou Gemini (Google Direto).")
            else:
                st.error(f"Ocorreu um erro t√©cnico: {erro_msg}")
