import streamlit as st
import os
from smolagents import CodeAgent, LiteLLMModel

# --- BLOCO DE IMPORTA√á√ÉO SEGURA (Corre√ß√£o para o erro da Nuvem) ---
try:
    import ollama
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False
    # Se estiver na nuvem (Streamlit Cloud), o Ollama n√£o carrega, e tudo bem!

# --- IMPORTS DOS NOSSOS M√ìDULOS ---
from config_carpintaria import CerebroHibrido
from ferramentas_avancadas import consultar_documentos, salvar_arquivo, ler_arquivo

# --- 1. CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(page_title="Carpintaria Digital Pro", page_icon="ü™ö", layout="wide")
st.title("ü™ö Carpintaria Digital Pro")

# Inicializa o C√©rebro
cerebro = CerebroHibrido()

# --- 2. PAINEL DE CONTROLE (SIDEBAR) ---
with st.sidebar:
    st.header("‚öôÔ∏è Painel de Controle")
    
    # Mostra status da conex√£o
    if cerebro.tem_internet:
        st.success(f"Sinal: {cerebro.modo}")
    else:
        st.warning(f"Sinal: {cerebro.modo}")

    st.subheader("üß† Modelo Base")
    modelo_local = st.selectbox(
        "Prefer√™ncia (Se local):", 
        ["qwen2.5-coder:3b", "llama3.2:latest", "phi3.5:latest"]
    )
    
    st.divider()
    modo_agente = st.toggle("üïµÔ∏è Ativar Modo Agente (Full Stack)", value=True)
    
    st.info("Ferramentas Ativas:\n- üìö RAG (Documentos)\n- üíæ Salvar Arquivos\n- üìñ Ler Arquivos")

    if st.button("üóëÔ∏è Limpar Mem√≥ria"):
        st.session_state["messages"] = []
        st.rerun()

# --- 3. HIST√ìRICO ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 4. L√ìGICA PRINCIPAL ---
if prompt := st.chat_input("Qual a tarefa de hoje?"):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()

        # Define qual configura√ß√£o usar (Nuvem ou Local automaticamente)
        config_llm = cerebro.obter_config_modelo(modelo_local)

        # --- MODO AGENTE (COM FERRAMENTAS E ACESSO AO DISCO) ---
        if modo_agente:
            status = st.status(f"üïµÔ∏è Agente processando com {config_llm['model_id']}...", expanded=True)
            try:
                # Configura o modelo dinamicamente
                modelo_agente = LiteLLMModel(
                    model_id=config_llm['model_id'],
                    api_base=config_llm['api_base'],
                    api_key=config_llm['api_key'],
                    max_tokens=4000,
                    temperature=0.2
                )

                # Inicializa Agente com FERRAMENTAS
                agent = CodeAgent(
                    tools=[consultar_documentos, salvar_arquivo, ler_arquivo], 
                    model=modelo_agente, 
                    add_base_tools=True,
                    additional_authorized_imports=['datetime', 'numpy', 'pandas', 'os', 'json']
                )

                # Prompt Refor√ßado para usar ferramentas
                prompt_sistema = f"""
                SOLICITA√á√ÉO: {prompt}

                DIRETRIZES:
                1. Se precisar de informa√ß√£o da empresa, USE 'consultar_documentos'.
                2. Se precisar criar c√≥digo, N√ÉO apenas mostre na tela. USE 'salvar_arquivo' para criar o arquivo real.
                3. Responda sempre em Portugu√™s.
                """

                resposta_final = agent.run(prompt_sistema)
                
                status.update(label="‚úÖ Tarefa Conclu√≠da!", state="complete", expanded=False)
                message_placeholder.markdown(resposta_final)
                st.session_state["messages"].append({"role": "assistant", "content": resposta_final})

            except Exception as e:
                status.update(label="‚ùå Erro", state="error")
                st.error(f"Erro no Agente: {e}")

        # --- MODO CHAT SIMPLES (FALLBACK OU CONVERSA R√ÅPIDA) ---
        else:
            # Verifica se o Ollama est√° dispon√≠vel antes de tentar usar
            if OLLAMA_AVAILABLE:
                full_response = ""
                for chunk in ollama.chat(model=modelo_local, messages=st.session_state["messages"], stream=True):
                    if 'message' in chunk and 'content' in chunk['message']:
                        full_response += chunk['message']['content']
                        message_placeholder.markdown(full_response + "‚ñå")
                message_placeholder.markdown(full_response)
                st.session_state["messages"].append({"role": "assistant", "content": full_response})
            else:
                # Se estiver na nuvem e tentar usar o modo simples (Ollama)
                msg_erro = "‚ö†Ô∏è O modo Chat Simples usa o Ollama (Local), que n√£o est√° dispon√≠vel na Nuvem. Por favor, ative o **Modo Agente** para usar Groq/Gemini."
                message_placeholder.warning(msg_erro)
                st.session_state["messages"].append({"role": "assistant", "content": msg_erro})
