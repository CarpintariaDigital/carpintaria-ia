import streamlit as st
import ollama
from smolagents import CodeAgent, LiteLLMModel

# --- IMPORTS DOS NOSSOS M√ìDULOS NOVOS ---
from config_carpintaria import CerebroHibrido
from ferramentas_avancadas import consultar_documentos, salvar_arquivo, ler_arquivo

# --- 1. CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(page_title="Carpintaria Digital Pro", page_icon="ü™ö", layout="wide")
st.title("ü™ö Carpintaria Digital Pro")

# Inicializa o C√©rebro
cerebro = CerebroHibrido()

# --- 2. PAINEL DE CONTROLE (SIDEBAR) ---
with st.sidebar:
    st.header("‚öôÔ∏è Painel de Controle")
    
    # Mostra status da conex√£o
    if cerebro.tem_internet:
        st.success(f"Sinal: {cerebro.modo}")
    else:
        st.warning(f"Sinal: {cerebro.modo}")

    st.subheader("üß† Modelo Base (Local)")
    modelo_local = st.selectbox(
        "Prefer√™ncia Offline:", 
        ["qwen2.5-coder:3b", "llama3.2:latest", "phi3.5:latest"]
    )
    
    st.divider()
    modo_agente = st.toggle("üïµÔ∏è Ativar Modo Agente (Full Stack)", value=True)
    
    st.info("Ferramentas Ativas:\n- üìö RAG (Documentos)\n- üíæ Salvar Arquivos\n- üìñ Ler Arquivos")

    if st.button("üóëÔ∏è Limpar Mem√≥ria"):
        st.session_state["messages"] = []
        st.rerun()

# --- 3. HIST√ìRICO ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 4. L√ìGICA PRINCIPAL ---
if prompt := st.chat_input("Qual a tarefa de hoje?"):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        # Define qual configura√ß√£o usar (Nuvem ou Local automaticamente)
        config_llm = cerebro.obter_config_modelo(modelo_local)
        
        # --- MODO AGENTE (COM FERRAMENTAS E ACESSO AO DISCO) ---
        if modo_agente:
            status = st.status(f"üïµÔ∏è Agente processando com {config_llm['model_id']}...", expanded=True)
            try:
                # Configura o modelo dinamicamente
                modelo_agente = LiteLLMModel(
                    model_id=config_llm['model_id'],
                    api_base=config_llm['api_base'],
                    api_key=config_llm['api_key'],
                    max_tokens=4000,
                    temperature=0.2
                )

                # Inicializa Agente com NOVAS FERRAMENTAS
                agent = CodeAgent(
                    tools=[consultar_documentos, salvar_arquivo, ler_arquivo], 
                    model=modelo_agente, 
                    add_base_tools=True,
                    additional_authorized_imports=['datetime', 'numpy', 'pandas', 'os', 'json']
                )

                # Prompt Refor√ßado para usar ferramentas
                prompt_sistema = f"""
                SOLICITA√á√ÉO: {prompt}
                
                DIRETRIZES:
                1. Se precisar de informa√ß√£o da empresa, USE 'consultar_documentos'.
                2. Se precisar criar c√≥digo, N√ÉO apenas mostre na tela. USE 'salvar_arquivo' para criar o arquivo real.
                3. Responda sempre em Portugu√™s.
                """

                resposta_final = agent.run(prompt_sistema)

                status.update(label="‚úÖ Tarefa Conclu√≠da!", state="complete", expanded=False)
                message_placeholder.markdown(resposta_final)
                st.session_state["messages"].append({"role": "assistant", "content": resposta_final})

            except Exception as e:
                status.update(label="‚ùå Erro", state="error")
                st.error(f"Erro no Agente: {e}")
                # Fallback: Se o agente falhar (ex: erro de internet na nuvem), tenta local
                st.warning("Tentando fallback para chat simples local...")

        # --- MODO CHAT SIMPLES (FALLBACK OU CONVERSA R√ÅPIDA) ---
        else:
            full_response = ""
            # No modo simples, for√ßamos o local Ollama para ser r√°pido
            for chunk in ollama.chat(model=modelo_local, messages=st.session_state["messages"], stream=True):
                if 'message' in chunk and 'content' in chunk['message']:
                    full_response += chunk['message']['content']
                    message_placeholder.markdown(full_response + "‚ñå")
            message_placeholder.markdown(full_response)
            st.session_state["messages"].append({"role": "assistant", "content": full_response})
