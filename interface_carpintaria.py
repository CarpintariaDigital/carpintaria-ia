import streamlit as st
import os
from smolagents import CodeAgent, LiteLLMModel

# --- BLOCO DE IMPORTA√á√ÉO SEGURA ---
try:
    import ollama
    # Tenta listar modelos s√≥ para ver se o servidor responde
    ollama.list()
    OLLAMA_AVAILABLE = True
except Exception:
    OLLAMA_AVAILABLE = False

# --- IMPORTS DOS NOSSOS M√ìDULOS ---
from ferramentas_avancadas import consultar_documentos, salvar_arquivo, ler_arquivo

# --- 1. CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(page_title="Carpintaria Digital Pro", page_icon="ü™ö", layout="wide")
st.title("ü™ö Carpintaria Digital Pro - Vers√£o H√≠brida")

# --- 2. PAINEL DE CONTROLE (SIDEBAR) ---
with st.sidebar:
    st.header("üß† C√©rebro da IA")
    
    # --- MENU DE ESCOLHA DE MODELOS ---
    # Dicion√°rio que liga o "Nome Bonito" ao "ID T√©cnico"
    # A estrutura √©: "Nome no Menu": ("provedor/modelo", "nome_da_variavel_api")
    
    opcoes_modelos = {
    # --- GOOGLE (Free Tier) ---
        "‚òÅÔ∏è Google: Gemini 1.5 Flash (R√°pido)": ("gemini/gemini-1.5-flash", "GEMINI_API_KEY"),
        "‚òÅÔ∏è Google: Gemini Pro (Est√°vel)": ("gemini/gemini-pro", "GEMINI_API_KEY"),
        
        # --- OPENROUTER (DeepSeek & Outros) ---
        "‚òÅÔ∏è OpenRouter: DeepSeek R1 (Racioc√≠nio Forte)": ("openrouter/deepseek/deepseek-r1-distill-llama-70b", "OPENROUTER_API_KEY"),
        "‚òÅÔ∏è OpenRouter: Mistral Large": ("openrouter/mistralai/mistral-large-2411", "OPENROUTER_API_KEY"),
        
        # --- GROQ ---
        "‚òÅÔ∏è Groq: Llama 3.3 (Vers√°til)": ("groq/llama-3.3-70b-versatile", "GROQ_API_KEY"),
    }

    # Se o Ollama estiver rodando (Local), adiciona as op√ß√µes locais
    if OLLAMA_AVAILABLE:
        opcoes_modelos["üè† Local: Qwen 2.5 Coder"] = ("ollama/qwen2.5-coder:3b", None)
        opcoes_modelos["üè† Local: Llama 3.2"] = ("ollama/llama3.2:latest", None)
        opcoes_modelos["üè† Local: Phi 3.5"] = ("ollama/phi3.5:latest", None)
        st.success("üü¢ Modo Local Ativo (Ollama Detectado)")
    else:
        st.info("‚òÅÔ∏è Modo Nuvem (Ollama Indispon√≠vel)")

    # O Menu Dropdown
    nome_escolhido = st.selectbox("Escolha o Modelo:", list(opcoes_modelos.keys()))
    
    # Pega as configura√ß√µes baseadas na escolha
    model_id, api_env_var = opcoes_modelos[nome_escolhido]

    st.divider()
    
    modo_agente = st.toggle("üïµÔ∏è Ativar Agente (Usa Ferramentas)", value=True)
    st.caption("Ferramentas: RAG (Docs), Salvar Arquivos, Ler Arquivos")

    if st.button("üóëÔ∏è Limpar Mem√≥ria"):
        st.session_state["messages"] = []
        st.rerun()

# --- 3. HIST√ìRICO ---
if "messages" not in st.session_state:
    st.session_state["messages"] = []

for msg in st.session_state["messages"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- 4. L√ìGICA PRINCIPAL ---
if prompt := st.chat_input("Como posso ajudar na carpintaria hoje?"):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        status = st.status(f"‚öôÔ∏è Processando com {nome_escolhido}...", expanded=True)

        try:
            # --- PREPARA√á√ÉO DAS CHAVES API ---
            api_key = None
            if api_env_var: # Se for modelo de nuvem
                api_key = os.environ.get(api_env_var)
                if not api_key:
                    status.update(label="‚ùå Erro de Chave", state="error")
                    st.error(f"Falta a chave {api_env_var} nos Secrets do Streamlit!")
                    st.stop()
            
            # --- CONFIGURA√á√ÉO DO MODELO ---
            modelo_agente = LiteLLMModel(
                model_id=model_id,
                api_key=api_key, # Pode ser None se for Ollama, o LiteLLM entende
                api_base="http://localhost:11434" if "ollama" in model_id else None,
                max_tokens=4000,
                temperature=0.2
            )

            # --- MODO AGENTE OU CHAT ---
            if modo_agente:
                agent = CodeAgent(
                    tools=[consultar_documentos, salvar_arquivo, ler_arquivo], 
                    model=modelo_agente, 
                    add_base_tools=True,
                    additional_authorized_imports=['datetime', 'numpy', 'pandas', 'os', 'json']
                )
                
                prompt_sistema = f"SOLICITA√á√ÉO: {prompt}\nContexto: Responda em Portugu√™s."
                resposta_final = agent.run(prompt_sistema)
            else:
                # Modo simples (sem ferramentas, mas usando o mesmo modelo selecionado)
                # Criamos um agente sem ferramentas s√≥ para conversar
                agent = CodeAgent(tools=[], model=modelo_agente, add_base_tools=False)
                resposta_final = agent.run(prompt)

            status.update(label="‚úÖ Conclu√≠do!", state="complete", expanded=False)
            message_placeholder.markdown(resposta_final)
            st.session_state["messages"].append({"role": "assistant", "content": resposta_final})

        except Exception as e:
            status.update(label="‚ùå Erro", state="error")
            st.error(f"Ocorreu um erro: {str(e)}")
            st.warning("Dica: Se for erro de 'Connection', verifique se o Ollama est√° rodando (se for local) ou as chaves API (se for nuvem).")
